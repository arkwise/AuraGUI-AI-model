Aura Ai Dataset collection for developing & Enhancing Aura GUI for FreeDOS. We have created 3/4 steps to parsse the code and then prepare it for pre-training and then training using transformers.

:Step A

This bash script is designed to analyze and extract key elements from header and source code files, such as functions, variables, constants, and more, to generate an output file containing the results. The extracted data is intended to be used for building or further processing in the next steps of a software development workflow.

How it Works:
	1.	User Input:
	•	The script prompts the user for two directories:
	•	HEADER_DIR: The directory containing header files (e.g., .h files).
	•	SOURCE_DIR: The directory containing source code files (e.g., .c, .cpp, or .java files).
	2.	Directory Validation:
	•	The script checks if the provided directories exist. If either directory is missing, the script will print an error and exit.
	3.	File Discovery:
	•	The script searches the directories for the relevant files:
	•	Header files (*.h) are found in the HEADER_DIR.
	•	Source files (*.c, *.cpp, *.java) are found in the SOURCE_DIR.
	4.	Output File Creation:
	•	A file named output.txt is created (or overwritten if it already exists). This file will store the results of the extraction.
	5.	Extraction Operations:
The script performs a series of extraction tasks using regular expressions (grep with the -E flag) and saves the results to individual text files, which are then appended to output.txt:
	•	Functions: Extracts function definitions from the source files.
	•	Variables: Extracts variable declarations (including extern, static, etc.) from the header files.
	•	Constants: Extracts constant definitions from the header files.
	•	Function Calls: Extracts function calls from the source files.
	•	Pointers: Extracts pointer declarations from the header files.
	•	Memory Allocations: Extracts memory allocation calls (e.g., malloc, calloc, realloc, free) from the source files.
	•	Conditional Compilation: Extracts conditional compilation blocks (e.g., #if, #ifdef, etc.) from the header files.
	•	Header Guards: Extracts #ifndef preprocessor directives used for header guards in the header files.
	•	Includes: Extracts #include preprocessor directives from the source files.
	•	Typedefs: Extracts typedef definitions from the header files.
	•	Enums: Extracts enum definitions from the header files.
	6.	Final Output:
	•	After all extraction tasks are completed, the script appends the results to the output.txt file. The script will print a message stating that the extraction is completed and the results are saved.

	:Step B

	The stepb.py script is designed to build a dataset from extracted code elements (such as functions, variables, constants, etc.) and map these elements to source files. It takes user input for directories, processes the extracted files, and generates a final JSON dataset for further use in subsequent stages of the workflow (such as step c).

How it Works:
	1.	User Input:
	•	The script prompts the user to provide paths for three directories:
	•	extracted_dir: The directory containing the text files (e.g., functions.txt, variables.txt) generated by the previous step.
	•	source_dir: The directory containing the source code files (e.g., .c, .cpp files).
	•	header_dir: The directory containing the header files (e.g., .h files).
	2.	Directory Validation:
	•	The script checks whether each of the provided directories exists. If any directory does not exist, an error message is printed, and the script stops.
	3.	Processing Extracted Files:
	•	The process_extracted_files function reads the extracted code elements from the text files (e.g., functions.txt, variables.txt, etc.) and stores them in a dictionary (data).
	4.	Mapping Code Elements to Source Files:
	•	The map_to_source function iterates through each function listed in the functions.txt file and searches for its occurrence in the source code files (.c and .cpp files) in the source_dir. If a match is found, the function is mapped to the source file it was found in.
	5.	Building the Dataset:
	•	The extracted data (functions, variables, constants, etc.) is combined with the file mappings and the header files directory path.
	•	A dictionary (dataset) is created to hold all the data:
	•	code_elements: The extracted data (functions, variables, etc.).
	•	file_mapping: The mapping of functions to source files.
	•	header_files: The path to the header files directory.
	6.	Saving the Dataset:
	•	The dataset is saved as a JSON file (dataset.json) for future use in the pipeline. The JSON file contains all the extracted code elements, their mappings to source files, and the header files directory.
	7.	Completion Message:
	•	Once the dataset is built and saved, a message is printed, indicating that the dataset has been successfully generated.


	:Step B1

	The stepb1.py script is designed to summarize and verify the dataset generated in the previous steps (such as stepb.py). It loads the dataset from a JSON file, checks its validity, and provides a detailed summary of the code elements, mappings, and header files. This step ensures that the dataset is correctly formatted and readable before proceeding to stepc.

How it Works:
	1.	Dataset Loading:
	•	The script begins by reading the dataset from a JSON file (dataset_file), which was generated in the previous step (e.g., stepb.py).
	•	The dataset is loaded into a dictionary structure (dataset) using the json.load() function.
	2.	Extracting Code Elements and Metadata:
	•	The script retrieves the code_elements and other relevant sections from the loaded dataset. These sections include:
	•	functions: The list of functions found in the source code.
	•	variables: The list of variables.
	•	constants: The list of constants.
	•	function_calls: The list of function calls.
	•	pointers: The list of pointer declarations.
	•	memory_allocations: The list of memory allocation functions.
	•	conditional_compilation: The list of conditional compilation blocks.
	•	header_guards: The list of header guards.
	•	includes: The list of include directives.
	•	typedefs: The list of typedef declarations.
	•	enums: The list of enums.
	•	It also retrieves the file_mapping (which maps functions to their respective source files) and header_files (the path to the header files directory).
	3.	Validity Checks:
	•	The script checks whether the code_elements section is empty. If it is, a message is printed, and the script stops. This ensures that there is valid data to summarize.
	4.	Summary Information:
	•	The script prints a summary of the dataset, including:
	•	Total count of each code element (functions, variables, constants, etc.).
	•	Number of file mappings.
	•	The header files directory path.
	5.	Sample Data:
	•	The script provides samples of the first 5 items from the following categories:
	•	Function names: Displays the first 5 functions in the dataset.
	•	Variables: Displays the first 5 variables.
	•	Enums: Displays the first 5 enums.
	•	File mappings: Displays the first 5 function-to-file mappings.
	6.	Purpose:
	•	This script is used to verify the dataset generated by the previous steps. It provides an overview of the extracted data and ensures that it is correctly formatted and readable.
	•	It helps to identify any issues in the dataset before proceeding to the next steps (such as stepc).

	:Step C

	The stepc.py script is designed to prepare the dataset generated in earlier steps and save it into a new file (prepared_dataset.json). This script processes each function in the dataset by constructing prompts and completions, then reads the source code to provide detailed responses for each function. The prepared dataset is saved in a format suitable for further use, possibly for fine-tuning a model.

How it Works:
	1.	User Input for Paths:
	•	The script starts by prompting the user to enter the paths for:
	•	Header files directory
	•	Source code directory
	•	These paths are necessary to locate the corresponding header files and source code files related to the functions in the dataset.
	2.	Directory Validation:
	•	The script checks if the provided directories for header files and source code exist.
	•	If either of the directories does not exist, it prints an error message and exits the script.
	3.	Reading Source Code:
	•	The script defines a helper function (read_source_code()) that attempts to open and read the contents of a file. If an error occurs (e.g., if the file is a directory or does not exist), an error message is returned.
	4.	Processing the Dataset:
	•	The script iterates over the file_mapping section of the dataset, which contains mappings between function signatures and the corresponding file paths.
	•	For each function signature:
	•	It checks the validity of the file path and determines if it refers to a header file or a source file.
	•	If the file exists, the script reads the source code from it.
	•	If the file is not found or the path is invalid, a placeholder message (// Source code not found or invalid file path) is used.
	5.	Preparing Input-Output Pairs:
	•	For each function, the script constructs an input-output pair:
	•	Input (Prompt): A question asking, “What does the function {function_signature} do?”
	•	Output (Completion): The source code (or error message) related to that function.
	•	These pairs are stored in a list.
	6.	Saving the Prepared Dataset:
	•	The script saves the list of input-output pairs to a new JSON file called prepared_dataset.json.
	7.	Error Handling:
	•	The script handles errors gracefully, including missing files or incorrect file paths, by printing informative messages and providing fallback responses when necessary.

Example Flow:
	1.	User Input:
	•	The user is prompted to enter paths for the header files and source code directories.
	2.	Dataset Processing:
	•	The script loads the dataset.json file, which contains the file_mapping of function signatures and their respective file paths.
	•	For each function, the script tries to read the corresponding file and prepares a prompt-completion pair.
	3.	Output:
	•	The resulting dataset (prepared_dataset.json) is saved, which contains the input-output pairs for each function.

	Key Points:
	•	Input: The script requires user input for the paths to header files and source code files.
	•	Output: The script produces a new JSON file (prepared_dataset.json) containing the function prompts and corresponding source code.
	•	Validation: It checks if the directories provided exist and handles missing or invalid files appropriately.
	•	Error Handling: If a file cannot be read or located, an error message is included in the output instead of the actual source code.

This script ensures that the dataset is correctly formatted, with clear and valid input-output pairs for each function, ready for further use such as training a model.